---
layout: default
title: Domain Generalization and Adaptation
---

# Domain Generalization and Adaptation
Machines learn from dataset repositories of examples generated by the humans. 
The examples are a creative expression of the label by the human. 
The predictive models that learn from examples therefore should be cautious not to entangle what is expressed (*label*) with how it is expressed (*style*). 
However, several machine learned models fall in to this trap and generalize poorly on benign style shifts of the distribution. 
Poor generalization to new domains has several practical consequences especially when deployed in the wild. 

Toward the objective of improving performance on any domain, we conduct research in the following two broad themes.  
**Domain Generalization**: When we have access to examples drawn from multiple domains (styles) during training, can we exploit the train-time domain variation to generalize better to any domain? \[[NeurIPS21](#neurips21), [ICML20](#icml20), [ICLR18](#iclr18)\].   
**Domain Adaptation**: How can we learn a model if we are interested in performance on a focussed target domain and are provided access to resources, albeit small, from the same domain? \[[Interspeech20](#interspeech20), [EMNLP20](#emnlp20), [ACL19](#acl19)\].

# Publications
 * Focus on the Common Good: Group Distributional Robustness Follows. \
   *Under Review*. V Piratla, P Netrapalli, S Sarawagi\
   \[[Paper](https://arxiv.org/pdf/2110.02619.pdf)\]
   <a name="neurips21"/>
 * Training for the Future: A Simple Gradient Interpolation Loss to Generalize Along Time. \
   In *NeurIPS 2021*. A Nasery, S Thakur, V Piratla, A De, S Sarawagi \
   \[[Paper](https://arxiv.org/pdf/2108.06721.pdf)\]  
   <a name="interspeech20"/>
 * Black-box Adaptation of ASR for Accented Speech. \
   In *Interspeech, 2020*. Kartik Khandelwal, Preethi Jyothi, Abhijeet Awasthi, Sunita Sarawagi\
   \[[Paper](https://arxiv.org/pdf/2006.13519)\] \[[Code](https://github.com/Kartik14/FineMerge)\]
   <a name="emnlp20"/>
 * NLP Service APIs and Models for Efficient Registration of New Clients.\
   In *Findings in EMNLP 2020*. S Shah, V Piratla, S Chakrabarti, S Sarawagi\
   \[[Paper](https://arxiv.org/pdf/2010.01526.pdf)\] \[[Code](https://github.com/sahil00199/KYC)\]
   <a name="icml20"/>
 * Efficient Domain Generalization via Common-Specific Low-Rank Decomposition. \
   In *ICML 2020*. V Piratla, P Netrapalli, S Sarawagi\
   \[[Paper](https://arxiv.org/pdf/2003.12815.pdf)\] \[[Code](https://github.com/vihari/CSD)\] \[[Talk ðŸ“¢](https://icml.cc/virtual/2020/poster/6528)\]
   <a name="acl19"/>
 * Topic-Sensitive Attention on Generic Corpora Corrects Sense Bias in Pretrained Embeddings.  In ACL, 2019. \
   In *ACL 2020*. V Piratla, S Sarawagi and S Chakrabarti\
   \[[Paper](https://aclanthology.org/P19-1168/)\] \[[Code](https://github.com/vihari/focussed_embs)\] \[[Talk ðŸ“¢](https://vimeo.com/384490539)\]
   <a name="iclr18"/>
 * Generalizing across domains via cross-gradient training. \
   In *ICLR 2018*.    S Shankar*, V Piratla*, S Chaudhuri, P Jyothi, S Chakrabarti, S Sarawagi \
   \[[Paper](https://arxiv.org/pdf/1804.10745.pdf)\] \[[Code](https://github.com/vihari/crossgrad)\]
   
# Collaborators
 * [Soumen Chakrabarti](https://www.cse.iitb.ac.in/~soumen/) 
 * [Siddhartha Chaudhuri](https://www.cse.iitb.ac.in/~sidch/)
 * [Abir De](https://abir-de.github.io/)
 * [Preethi Jyothi](https://www.cse.iitb.ac.in/~pjyothi/)
 * [Praneeth Netrapalli](http://praneethnetrapalli.org/)
 
